{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de14107bf5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def bias(exp_values,y_test):\n",
    "\tans=0.00\n",
    "\tfor i in range(len(y_test)):\n",
    "\t\ttemp=(y_test[i]-np.mean(exp_values[:,i]))\n",
    "\t\ttemp*=temp\n",
    "\t\tans+=temp\n",
    "\tans/=len(y_test)\n",
    "\treturn ans \n",
    "\n",
    "def variance(exp_values,y_test):\n",
    "\tans=0.0\n",
    "\tfor i in range(500):\n",
    "\t\t# print(exp_values[i].shape)\n",
    "\t\tans+=(np.var(exp_values[:,i]))\n",
    "\tans/=500\n",
    "\treturn ans\n",
    "\t\n",
    "\n",
    "def main():\n",
    "\tdatafile = open('./Q1_data/data.pkl',\"rb\");\n",
    "\tdataset = pickle.load(datafile)\n",
    "\tx=dataset[:,:-1]\n",
    "\ty=dataset[:,-1]\n",
    "\ty_dum=len(y)\n",
    "\t# print(y_dum)\n",
    "\ty=y.reshape(y_dum,1)\n",
    "\tx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=0)\n",
    "\tsz1=len(x_train)\n",
    "\tsz2=sz1/10\n",
    "\tsz2=int(sz2)\n",
    "\tx_test=x_test.reshape(len(x_test),1)\n",
    "\tx_train=x_train.reshape(len(x_train),1)\n",
    "\ty_test=y_test.reshape(len(y_test),1)\n",
    "\ty_train=y_train.reshape(len(y_train),1)\n",
    "\tx_train_subs=np.empty(shape=(10,sz2,1))\n",
    "\ty_train_subs=np.empty(shape=(10,sz2,1))\n",
    "\tfor i in range(9):\n",
    "\t\tif(i==0):\n",
    "\t\t    y1=y_train\n",
    "\t\t    x12=x_train\n",
    "\t\tsz3=sz2/sz1\n",
    "\t\t# print(sz3,sz1)\n",
    "\t\tx22,x_train_subs[i],y2,y_train_subs[i]=train_test_split(x12,y1,test_size=sz3,random_state=0)\n",
    "\t\tsz1-=sz2\n",
    "\t\ty1=y2\n",
    "\t\tx12=x22\n",
    "\t\t# print(x12.shape,y1.shape)\n",
    "\t# print(x12.shape,y1.shape)\n",
    "\t# print(x12)\n",
    "\tx_train_subs[9]=x12\n",
    "\ty_train_subs[9]=y1\n",
    "\treg=LinearRegression()\n",
    "\tbias_ar=np.empty(shape=(9,1))\n",
    "\tvar_ar=np.empty(shape=(9,1))\n",
    "\tdeg=np.empty(shape=(9,1))\n",
    "\tprint(\"degree bias variance\")\n",
    "\tfor i in range(1,10):\n",
    "\t\tdeg[i-1]=i\n",
    "\t\tprint(i,end=\" \")\n",
    "\t\texp_values=np.empty(shape=(10,len(x_test),1))\n",
    "\t\tif(i>1):\n",
    "\t\t\tpoly_fet = PolynomialFeatures(degree=i)\n",
    "\t\tfor i1 in range(10):\n",
    "\t\t\tif(i==1):\n",
    "\t\t\t\tx_poly=x_train_subs[i1]\n",
    "\t\t\t\tx_tst=x_test\n",
    "\t\t\telse:\n",
    "\t\t\t\tx_poly=poly_fet.fit_transform(x_train_subs[i1])\n",
    "\t\t\t\tx_tst=poly_fet.fit_transform(x_test)\n",
    "\t\t\treg.fit(x_poly,y_train_subs[i1])\n",
    "\t\t\t# print(reg.coef_)\n",
    "\t\t\texp_values[i1]=reg.predict(x_tst)\n",
    "\n",
    "\t\tmean_bias_sq=bias(exp_values,y_test)\n",
    "\t\tmean_variance=variance(exp_values,y_test)\n",
    "\t\tbias_ar[i-1]=mean_bias_sq\n",
    "\t\tvar_ar[i-1]=mean_variance\n",
    "\t\tprint(mean_bias_sq,mean_variance)\n",
    "\n",
    "\tplt.plot(deg,bias_ar)\n",
    "\tplt.title(\"Bias^2 vs Degree\")\n",
    "\tplt.xlabel(\"Degree of Polynomial\")\n",
    "\tplt.ylabel(\"Bias^2\")\n",
    "\tplt.show()\n",
    "\n",
    "\tplt.plot(deg,var_ar)\n",
    "\tplt.title(\"Variance vs Degree\")\n",
    "\tplt.xlabel(\"Degree of Polynomial\")\n",
    "\tplt.ylabel(\"Variance\")\n",
    "\tplt.show()\n",
    "\t# plt.title(\"Bias\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
